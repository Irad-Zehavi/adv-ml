{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp input_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While other implementations (such as `advertorch`) exist, this one is meant to be as easy, accessible, informative and modular as training model. In fact, this implementation uses `fastai`'s `Learner` class, and inherits its functionality, such as the progress bar, the losses table, and even early stopping and lr scheduling.\n",
    "Useful for evasion and feature-collision attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Type\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from torch import nn\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "class PerturbationCallback(ABC, Callback):\n",
    "    \"Manages the input perturbation for an `InputOptimizer`\"\n",
    "    @abstractmethod\n",
    "    def init_pert(self, x):\n",
    "        ...\n",
    "    \n",
    "\n",
    "class _Perturbation(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.p\n",
    "\n",
    "                \n",
    "class _TrainLoop(TrainEvalCallback):\n",
    "    def before_train(self):\n",
    "        super().before_train()\n",
    "        self.model.eval()\n",
    "\n",
    "    def before_validate(self):\n",
    "        raise CancelValidException\n",
    "\n",
    "\n",
    "class InputOptimizer(object):\n",
    "    \"Constructs adversarial examples: slightly perturbed inputs that fool classification models\"\n",
    "    def __init__(self,\n",
    "                 model: Module,\n",
    "                 pert_cb: Type[PerturbationCallback],\n",
    "                 loss: Callable = CrossEntropyLossFlat(),\n",
    "                 lr: float = None,  # pass `None` to try pick `lr` based on other parameters\n",
    "                 targeted: bool = False,  # Whether the constructed inputs should be classified as the specified targets or not\n",
    "                 min_delta: float = 1e-2,  # Minimum loss delta for `ReduceLROnPlateau` and `EarlyStoppingCallback`\n",
    "                 min_lr: float = 1e-6,  # Minimum lr for `ReduceLROnPlateau`\n",
    "                 # defaults taken from advertorch\n",
    "                 epoch_size: int = 10,  # Affects how often epoch-callbacks are called (e.g. `Recorder`` and `EarlyStoppingCallback`)\n",
    "                 n_epochs: int = 4):\n",
    "        self.loss = loss if targeted else (lambda *args, **kwargs: -loss(*args, **kwargs))\n",
    "        store_attr('model, pert_cb, lr, min_delta, min_lr, epoch_size, n_epochs')\n",
    "        \n",
    "        if self.lr is None:\n",
    "            assert hasattr(pert_cb, 'epsilon'), \"Can't pick lr for if the callback isn't based on an epsilon\"\n",
    "            self.lr = pert_cb.epsilon / self.epoch_size\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.requires_grad_(False)\n",
    "\n",
    "    def perturb(self, dsets):\n",
    "        x, y = dsets.load()\n",
    "        x, y = x.detach().clone(), y.detach().clone()  # TODO: can I get rid of this?\n",
    "\n",
    "        x_hat = self._perturb(x, y)\n",
    "\n",
    "        return Datasets(tls=[TfmdLists(x_hat, ToTensor()),  # ToTensor for decoding\n",
    "                             dsets.tls[1]])\n",
    "\n",
    "    def _perturb(self, x, y):\n",
    "        self.pert_cb.init_pert(x)\n",
    "\n",
    "        learner = Learner(DataLoaders([(x, y) for _ in range(self.epoch_size)], []),\n",
    "                          nn.Sequential(_Perturbation(self.pert_cb.p), self.model),\n",
    "                          self.loss,\n",
    "                          SGD,\n",
    "                          self.lr,\n",
    "                          train_bn=False,\n",
    "                          default_cbs=False,\n",
    "                          cbs=[_TrainLoop, Recorder(valid_metrics=False), ProgressCallback, BnFreeze,\n",
    "                               self.pert_cb,\n",
    "                               ReduceLROnPlateau('train_loss', min_delta=self.min_delta, min_lr=self.min_lr),\n",
    "                               EarlyStoppingCallback('train_loss', min_delta=self.min_delta / 10)\n",
    "                               ])\n",
    "        learner.fit(self.n_epochs)\n",
    "        return x.cpu() + self.pert_cb.p.data.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### InputOptimizer.perturb\n",
       "\n",
       ">      InputOptimizer.perturb (dsets)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### InputOptimizer.perturb\n",
       "\n",
       ">      InputOptimizer.perturb (dsets)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(InputOptimizer.perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
