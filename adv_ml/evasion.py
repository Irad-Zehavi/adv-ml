# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/pgd.ipynb.

# %% auto 0
__all__ = ['LinfProjectionCallback', 'PGD']

# %% ../nbs/pgd.ipynb 3
from advertorch.attacks import LinfPGDAttack
from typing import Callable

import torch
from fastai.vision.all import *

from torch.nn import Module, Sequential
from torch.nn.parameter import Parameter


class LinfProjectionCallback(Callback):
    def __init__(self, epsilon, x, rand_init):
        super().__init__()
        self.epsilon = epsilon
        self.p = Parameter(torch.rand(x.shape) * self.epsilon if rand_init else torch.zeros(x.shape))
        self.clamp_pixel_values(x)

    def before_step(self):
        with torch.no_grad():
            self.p.grad.sign_()

    def after_batch(self):
        with torch.no_grad():
            self.p.clamp_(-self.epsilon, self.epsilon)  # keep perturbation small
            self.clamp_pixel_values(self.x)

    def clamp_pixel_values(self, x):
        with torch.no_grad():
            x = x.to(self.p.device)
            self.p.data = (x + self.p).clamp(0., 1.) - x


class PGD(object):
    def __init__(self,
                 model: Module,
                 loss: Callable,
                 targeted: bool,  # Whether the constructed inputs should be classified as the specified targets or not
                 min_delta: float = 1e-2,  # Minimum loss delta for `ReduceLROnPlateau` and `EarlyStoppingCallback`
                 min_lr: float = 1e-6,  # Minimum lr for `ReduceLROnPlateau`
                 lr: float = None,  # pass `None` to pick `lr` based on other params
                 # defaults taken from advertorch
                 epsilon: float = 0.3,
                 epoch_size: int = 10,  # Affects how often epoch-callbacks are called (e.g. `Recorder`` and `EarlyStoppingCallback`)
                 n_epochs: int = 4,
                 rand_init: bool = True):
        self.loss = loss if targeted else (lambda *args, **kwargs: -loss(*args, **kwargs))
        self.lr = lr or (epsilon / epoch_size)
        store_attr('model, min_delta, min_lr, epsilon, epoch_size, n_epochs, rand_init')

    def perturb_dl(self, dl: DataLoader):
        return torch.cat([self.perturb_batch(x, y) for x, y in dl])

    def perturb(self, dsets):
        x, y = dsets.load()
        x, y = x.detach().clone(), y.detach().clone()  # TODO: can I get rid of this?
        self.model.eval()
        self.model.requires_grad_(False)

        proj_callback = LinfProjectionCallback(self.epsilon, x, self.rand_init)
        pert_module = Lambda(proj_callback.p.add)
        pert_module.p = proj_callback.p  # to register as a parameter

        class TrainLoop(TrainEvalCallback):
            def before_train(self):
                super().before_train()
                self.model.eval()

            def before_validate(self):
                raise CancelValidException

        learner = Learner(DataLoaders([(x, y) for _ in range(self.epoch_size)], []),
                          Sequential(pert_module, self.model),
                          self.loss,
                          SGD,
                          self.lr,
                          train_bn=False,
                          default_cbs=False,
                          cbs=[TrainLoop, Recorder(valid_metrics=False), ProgressCallback, BnFreeze,
                               proj_callback,
                               ReduceLROnPlateau('train_loss', min_delta=self.min_delta, min_lr=self.min_lr),
                               EarlyStoppingCallback('train_loss', min_delta=self.min_delta / 10)
                               ])
        learner.fit(self.n_epochs)
        p = proj_callback.p.data.detach().cpu()
        return Datasets(tls=[TfmdLists(x.cpu() + p, ToTensor()),  # ToTensor for decoding
                             dsets.tls[1]])
