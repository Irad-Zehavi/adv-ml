# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/pgd.ipynb.

# %% auto 0
__all__ = ['LinfProjectionCallback', 'PGD']

# %% ../nbs/pgd.ipynb 3
from advertorch.attacks import LinfPGDAttack
from dataclasses import dataclass
from typing import Callable

import torch
from fastai.vision.all import *

from torch.nn import Module, Sequential
from torch.nn.parameter import Parameter


class LinfProjectionCallback(Callback):
    def __init__(self, epsilon, x, rand_init):
        super().__init__()
        self.epsilon = epsilon
        self.p = Parameter(torch.rand(x.shape) * self.epsilon if rand_init else torch.zeros(x.shape))
        self.clamp_pixel_values(x)

    def before_step(self):
        with torch.no_grad():
            self.p.grad.sign_()

    def after_batch(self):
        with torch.no_grad():
            self.p.clamp_(-self.epsilon, self.epsilon)  # keep perturbation small
            self.clamp_pixel_values(self.x)

    def clamp_pixel_values(self, x):
        with torch.no_grad():
            x = x.to(self.p.device)
            self.p.data = (x + self.p).clamp(0., 1.) - x


@dataclass
class PGD(object):
    model: Module
    loss: Callable
    min_delta: float = 1e-2
    min_lr: float = 1e-6
    lr: float = None
    # defaults taken from advertorch
    epsilon: float = 0.3
    epoch_size: int = 10
    n_epochs: int = 4
    rand_init: bool = True

    def __post_init__(self):
        if self.lr is None:
            self.lr = self.epsilon / self.epoch_size

    def perturb_dl(self, dl: DataLoader):
        return torch.cat([self.perturb_batch(x, y) for x, y in dl])

    def perturb(self, x, y):
        x, y = x.detach().clone(), y.detach().clone()  # TODO: can I get rid of this?
        self.model.eval()
        self.model.requires_grad_(False)

        proj_callback = LinfProjectionCallback(self.epsilon, x, self.rand_init)
        pert_module = Lambda(proj_callback.p.add)
        pert_module.p = proj_callback.p  # to register as a parameter

        class TrainLoop(TrainEvalCallback):
            def before_train(self):
                super().before_train()
                self.model.eval()

            def before_validate(self):
                raise CancelValidException

        learner = Learner(Datasets(range(self.epoch_size), [lambda _: x, lambda _: y]).dataloaders(bs=None, drop_last=False, shuffle=False),
                          Sequential(pert_module, self.model),
                          CrossEntropyLossFlat(),
                          SGD,
                          self.lr,
                          train_bn=False,
                          default_cbs=False,
                          cbs=[TrainLoop, Recorder(valid_metrics=False), ProgressCallback, BnFreeze,
                               proj_callback,
                               ReduceLROnPlateau('train_loss', min_delta=self.min_delta, min_lr=self.min_lr),
                               EarlyStoppingCallback('train_loss', min_delta=self.min_delta / 10)
                               ])
        learner.fit(self.n_epochs)
        p = proj_callback.p.data.detach().cpu()
        return x.cpu() + p
