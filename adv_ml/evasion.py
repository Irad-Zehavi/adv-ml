# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/evasion.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/evasion.ipynb 3
from abc import ABC, abstractmethod
from typing import Callable

import torch
from fastai.vision.all import *

from torch.nn import Module, Sequential
from torch.nn.parameter import Parameter


class Perturber(ABC, Module, Callback):
    ...


class LinfPerturber(Perturber):
    def __init__(self, epsilon, x, rand_init):
        super().__init__()
        self.epsilon = epsilon
        self.p = Parameter(torch.rand(x.shape) * self.epsilon if rand_init else torch.zeros(x.shape))
        self.clamp_pixel_values(x)

    def forward(self, x):
        print(f'x: {x}')
        print(f'p: {self.p}')
        return x + self.p

    def before_step(self):
        with torch.no_grad():
            self.p.grad.sign_()

    def after_batch(self):
        with torch.no_grad():
            self.p.clamp_(-self.epsilon, self.epsilon)  # keep perturbation small
            self.clamp_pixel_values(self.x)

    def clamp_pixel_values(self, x):
        with torch.no_grad():
            x = x.to(self.p.device)
            self.p.data = (x + self.p).clamp(0., 1.) - x


class InputPerturbationOptimizer(ABC):
    "Optimizes inputs to a model according to some loss function"
    def __init__(self,
                 model: Module,
                 loss: Callable,
                 perturber_type: type,  # A `Callback` responsible for projection
                 targeted: bool,  # Whether the constructed inputs should be classified as the specified targets or not
                 lr: float = None,  # pass `None` to pick `lr` based on other params
                 min_delta: float = 1e-2,  # Minimum loss delta for `ReduceLROnPlateau` and `EarlyStoppingCallback`
                 min_lr: float = 1e-6,  # Minimum lr for `ReduceLROnPlateau`
                 # defaults taken from advertorch
                 epsilon: float = 0.3,
                 epoch_size: int = 10,  # Affects how often epoch-callbacks are called (e.g. `Recorder`` and `EarlyStoppingCallback`)
                 n_epochs: int = 4,
                 rand_init: bool = True):
        self.loss = loss if targeted else (lambda *args, **kwargs: -loss(*args, **kwargs))
        self.lr = lr or (epsilon / epoch_size)
        store_attr('model, perturber_type, min_delta, min_lr, epsilon, epoch_size, n_epochs, rand_init')

    def perturb(self, dsets):
        x, y = dsets.load()
        x, y = x.detach().clone(), y.detach().clone()  # TODO: can I get rid of this?
        self.model.eval()
        self.model.requires_grad_(False)

        class TrainLoop(TrainEvalCallback):
            def before_train(self):
                super().before_train()
                self.model.eval()

            def before_validate(self):
                raise CancelValidException

        perturber = self.perturber_type(self.epsilon, x, self.rand_init)

        learner = Learner(DataLoaders([(x, y) for _ in range(self.epoch_size)], []),
                          Sequential(perturber, self.model),
                          self.loss,
                          SGD,
                          self.lr,
                          train_bn=False,
                          default_cbs=False,
                          cbs=[TrainLoop, Recorder(valid_metrics=False), ProgressCallback, BnFreeze,
                               perturber,
                               ReduceLROnPlateau('train_loss', min_delta=self.min_delta, min_lr=self.min_lr),
                               EarlyStoppingCallback('train_loss', min_delta=self.min_delta / 10)
                               ])
        learner.fit(self.n_epochs)
        p = self.perturber.p.data.detach().cpu()
        
        return Datasets(tls=[TfmdLists(x.cpu() + p, ToTensor()),  # ToTensor for decoding
                             dsets.tls[1]])
